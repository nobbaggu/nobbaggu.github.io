I"¹6<p>ë¨¸ì‹ ëŸ¬ë‹ competition ìœ¼ë¡œ ìœ ëª…í•œ í”Œë«í¼ì¸ Kaggleì— ì…ë¬¸í•  ë•Œ ê°€ì¥ ë§ì´ ë‹¤ë£¨ëŠ” titanic survival dataë¡œ êµ¬ì¶œì ì˜ˆì¸¡ì— ê´€í•œ ëª¨ë¸ë§ì„ í•´ë³´ì•˜ë‹¤. ì´ datasetì—ëŠ” ë¶„ëª… patternì´ë€ ê²ƒì´ ë‚˜íƒ€ë‚œë‹¤. êµ¬ì¶œëœ ì‚¬ëŒì˜ ë°ì´í„°ì—ëŠ” ë‚¨ìë³´ë‹¤ëŠ” ì—¬ì, ì²­ë…„ë³´ë‹¤ëŠ” ë…¸ì¸ê³¼ ì–´ë¦°ì•„ì´, ë” ë†’ì€ ê°ì„ì˜ ë“±ê¸‰ì´ë¼ëŠ” íŠ¹ì§•ì´ ìˆë‹¤. ì´ datasetìœ¼ë¡œ ëª¨ë¸ë§ì„ í•˜ë©´ ì´í›„ì—ë„ ìŠ¹ê°ì˜ ë°ì´í„°ë¥¼ ê°€ì§€ê³  êµ¬ì¶œì´ ë  ê²ƒì¸ì§€ ì•„ë‹Œì§€ ì˜ˆì¸¡ì„ í•  ìˆ˜ ìˆë‹¤.</p>

<pre><span style="color: #0000ff;">
from</span> pandas <span style="color: #0000ff;">import</span> read_csv
<span style="color: #0000ff;">from</span> pandas <span style="color: #0000ff;">import</span> set_option
<span style="color: #0000ff;">from</span> numpy <span style="color: #0000ff;">import</span> set_printoptions
<span style="color: #0000ff;">from</span> matplotlib <span style="color: #0000ff;">import</span> pyplot
<span style="color: #0000ff;">from</span> pandas.plotting <span style="color: #0000ff;">import</span> scatter_matrix
<span style="color: #0000ff;">from</span> sklearn.feature_selection <span style="color: #0000ff;">import</span> SelectKBest
<span style="color: #0000ff;">from</span> sklearn.feature_selection <span style="color: #0000ff;">import</span> chi2
<span style="color: #0000ff;">from</span> sklearn.preprocessing <span style="color: #0000ff;">import</span> MinMaxScaler
<span style="color: #0000ff;">from</span> sklearn.preprocessing <span style="color: #0000ff;">import</span> StandardScaler
<span style="color: #0000ff;">from</span> sklearn.model_selection <span style="color: #0000ff;">import</span> train_test_split
<span style="color: #0000ff;">from</span> sklearn.linear_model <span style="color: #0000ff;">import</span> LogisticRegression as LR
<span style="color: #0000ff;">from</span> sklearn.linear_model <span style="color: #0000ff;">import</span> Ridge
<span style="color: #0000ff;">from</span> sklearn.discriminant_analysis <span style="color: #0000ff;">import</span> LinearDiscriminantAnalysis as LDA
<span style="color: #0000ff;">from</span> sklearn.decomposition <span style="color: #0000ff;">import</span> PCA
<span style="color: #0000ff;">from</span> sklearn.neighbors <span style="color: #0000ff;">import</span> KNeighborsClassifier as KNC
<span style="color: #0000ff;">from</span> sklearn.pipeline <span style="color: #0000ff;">import</span> FeatureUnion
<span style="color: #0000ff;">from</span> sklearn.pipeline <span style="color: #0000ff;">import</span> Pipeline
<span style="color: #0000ff;">from</span> sklearn.model_selection <span style="color: #0000ff;">import</span> KFold
<span style="color: #0000ff;">from</span> sklearn.model_selection <span style="color: #0000ff;">import</span> cross_val_score
<span style="color: #0000ff;">from</span> sklearn.ensemble <span style="color: #0000ff;">import</span> RandomForestClassifier as RFC
<span style="color: #0000ff;">from</span> sklearn.ensemble <span style="color: #0000ff;">import</span> BaggingClassifier
<span style="color: #0000ff;">from</span> sklearn.externals.joblib <span style="color: #0000ff;">import</span> dump
<span style="color: #0000ff;">from</span> sklearn.externals.joblib <span style="color: #0000ff;">import</span> load
<span style="color: #0000ff;">import</span> pandas <span style="color: #0000ff;">as</span> pd
<span style="color: #0000ff;">import</span> numpy <span style="color: #0000ff;">as</span> np
<span style="color: #0000ff;">from</span> sklearn.model_selection <span style="color: #0000ff;">import</span> RandomizedSearchCV
<span style="color: #0000ff;">from</span> scipy.stats <span style="color: #0000ff;">import</span> uniform
<span style="color: #0000ff;">from</span> sklearn.metrics <span style="color: #0000ff;">import</span> accuracy_score

<span style="color: #999999;">#### data load ####</span>
filename = <span style="color: #3b9931;">'titanic_survival_train.csv'</span>
names = []
data = read_csv(filename)
data.pop(<span style="color: #3b9931;">'PassengerId'</span>)  <span style="color: #999999;"># delete 4 unrelated data</span>
data.pop(<span style="color: #3b9931;">'Name'</span>)
data.pop(<span style="color: #3b9931;">'Cabin'</span>)
data.pop(<span style="color: #3b9931;">'Ticket'</span>)

print(data.isnull().any())  <span style="color: #999999;"># NaN data verification</span>

sex_mapping = {<span style="color: #3b9931;">'male'</span>: <span style="color: #993300;">1</span>, <span style="color: #3b9931;">'female'</span>: <span style="color: #993300;">2</span>}  <span style="color: #999999;"># character data -&gt; numeric data</span>
data[<span style="color: #3b9931;">'Sex'</span>] = data[<span style="color: #3b9931;">'Sex'</span>].map(sex_mapping)

embark_mapping = {<span style="color: #3b9931;">'C'</span>: <span style="color: #993300;">1</span>, <span style="color: #3b9931;">'S'</span>: <span style="color: #993300;">2</span>, <span style="color: #3b9931;">'Q'</span>: <span style="color: #993300;">3</span>}  <span style="color: #999999;"># character data -&gt; numeric data</span>
data[<span style="color: #3b9931;">'Embarked'</span>] = data[<span style="color: #3b9931;">'Embarked'</span>].map(embark_mapping)

data.Age = data.Age.fillna(pd.DataFrame.median(data.Age))  <span style="color: #999999;"># NaN -&gt; median value</span>
data.Embarked = data.Embarked.fillna(<span style="color: #993300;">2</span>)  <span style="color: #999999;"># NaN -&gt; highest freuquent value</span>

array = data.values
Y = array[:, 0]
X = array[:, 1:]

<span style="color: #999999;">#### data statistics ####</span>
set_option(<span style="color: #3b9931;">'display.width'</span>, <span style="color: #993300;">100</span>)
set_option(<span style="color: #3b9931;">'precision'</span>, <span style="color: #993300;">2</span>)
<span style="color: #800080;">print</span>(data.describe())
<span style="color: #800080;">print</span>(data.skew())
<span style="color: #800080;">print</span>(data.corr(method=<span style="color: #3b9931;">'pearson'</span>))
<span style="color: #800080;">print</span>(data.groupby(<span style="color: #3b9931;">'Survived'</span>).size())

<span style="color: #999999;">#### visualization of data ####</span>
data.plot(kind=<span style="color: #3b9931;">'density'</span>, subplots=<span style="color: #800080;">True</span>, layout=(3, 3), sharex=<span style="color: #800080;">False</span>)
pyplot.show()
data.plot(kind=<span style="color: #3b9931;">'box'</span>, subplots=<span style="color: #800080;">True</span>, layout=(3, 3), sharex=<span style="color: #800080;">False</span>, sharey=<span style="color: #800080;">False</span>)
pyplot.show()
scatter_matrix(data)
pyplot.show()

<span style="color: #999999;">#### feature scaling ####</span>
scaler = MinMaxScaler(feature_range=(<span style="color: #993300;"></span>, <span style="color: #993300;">1</span>))
rescaledX = scaler.fit_transform(X)
set_printoptions(precision=3)
X = rescaledX
<span style="color: #800080;">print</span>(X)


<span style="color: #999999;">#### feature selection ####</span>
kbest = SelectKBest(score_func=chi2, k=<span style="color: #993300;">4</span>)
fit = kbest.fit(X, Y)
features = fit.transform(X)
set_printoptions(precision=3)
X = features
<span style="color: #800080;">print</span>(X)

X_tr, X_test, Y_tr, Y_test = train_test_split(X, Y, test_size=<span style="color: #993300;">0.2</span>, random_state=<span style="color: #993300;">7</span>)
X = X_tr
Y = Y_tr

<span style="color: #3b9931;">"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""</span>
<span style="color: #3b9931;">########pipelineì„ í†µí•œ feature selection &amp; model selection###########</span>
<span style="color: #3b9931;">features = []</span>
<span style="color: #3b9931;">features.append(('KBest', SelectKBest(score_func=chi2, k=3))) #KBest ì•Œê³ ë¦¬ì¦˜ê³¼ LDA ì•Œê³ ë¦¬ì¦˜ ë¹„êµ</span>
<span style="color: #3b9931;">features.append(('lda', LDA()))</span>
<span style="color: #3b9931;">feature_union = FeatureUnion(features) #ë” ì¢‹ì€ ì ìˆ˜ë¥¼ ì–»ì€ feature selection ì•Œê³ ë¦¬ì¦˜ ì„ íƒ</span>
<span style="color: #3b9931;">models = [('LogisticRegression', LR()), ('K-Nearest Neighbors', KNC()), ('LDA', LDA())]#Linear Regression, K-Nearest Neighbors, LDA ì•Œê³ ë¦¬ì¦˜ ë¹„êµ</span>
<span style="color: #3b9931;">kfold = KFold(n_splits=10, random_state=7) # 10ê°œì˜ fold dataset</span>
<span style="color: #3b9931;">for name, model in models:</span>
<span style="color: #3b9931;">   estimators=[]</span>
<span style="color: #3b9931;">   estimators.append(('feature_union', feature_union))</span>
<span style="color: #3b9931;">   estimators.append((name, model))</span>
<span style="color: #3b9931;">   model_set = Pipeline(estimators) #pipelineì„ í†µí•œ ê°€ì¥ ì¢‹ì€ ëª¨ë¸ ì„ íƒ</span>
<span style="color: #3b9931;">   result = cross_val_score(model_set, X, Y, cv=kfold)</span>
<span style="color: #3b9931;">   print('%s: %.3f%% (%.3f%%)' %(name, result.mean()*100, result.std()*100)) #accuracy score</span>
<span style="color: #3b9931;">"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""</span>

models = []
models.append((<span style="color: #3b9931;">'LR'</span>, LR()))
models.append((<span style="color: #3b9931;">'KNN'</span>, KNC()))
models.append((<span style="color: #3b9931;">'RF'</span>, RFC(n_estimators=<span style="color: #993366;">100</span>, max_features=<span style="color: #993366;">4</span>)))

kfold = KFold(n_splits=<span style="color: #993366;">10</span>, random_state=<span style="color: #993366;">7</span>)  <span style="color: #999999;"># 10ê°œì˜ fold dataset</span>
<span style="color: #0000ff;">for</span> name, model <span style="color: #0000ff;">in</span> models:
    results = cross_val_score(model, X, Y, cv=kfold)
    print(<span style="color: #3b9931;">'%s cross validation Accuracy: %.3f%% (%.3f%%)'</span> % (name, results.mean() * <span style="color: #993366;">100</span>, results.std() * <span style="color: #993366;">100</span>))
<span style="color: #999999;">#### K-Nearest Neighbors Classifer modelì´ ê°€ì¥ ì¢‹ì€ ì ìˆ˜</span>

</pre>

<pre><span style="color: #0000ff;">for</span> name, model <span style="color: #0000ff;">in</span> models:
    model.fit(X,Y)
    prediction = model.predict(X_test)
    print(<span style="color: #3b9931;">'%s test accuracy: %.3f%%'</span> %(name, accuracy_score(Y_test, prediction)*<span style="color: #993366;">100</span>))</pre>

<pre><span style="color: #999999;">#### Improve Model Performance(hyper-parameter tuning, ensemble method) ####</span>
<span style="color: #999999;"> # model = BaggingClassifier(base_estimator=LR(), n_estimators=100, random_state=7)</span>
<span style="color: #999999;"> # results = cross_val_score(model, X, Y, cv=kfold)</span>
<span style="color: #999999;"> # Bagging_result = results</span>
<span style="color: #999999;"> #</span>
<span style="color: #999999;"> # num_trees = 100</span>
<span style="color: #999999;"> # max_features = 3</span>
<span style="color: #999999;"> # model = RFC(n_estimators = num_trees, max_features = max_features)</span>
<span style="color: #999999;"> # results = cross_val_score(model, X, Y, cv=kfold)</span>
<span style="color: #999999;"> # RandomForest_result = results</span>
<span style="color: #999999;"> #</span>
<span style="color: #999999;"> # print('LR Accuracy: %.3f%%' %(LR_result.mean()*100))</span>
<span style="color: #999999;"> # print('LR Bagging Accuracy: %.3f%%' %(Bagging_result.mean()*100))</span>
<span style="color: #999999;"> # print('RandomForest Accuracy: %.3f%%' %(RandomForest_result.mean()*100)) </span> 


<span style="color: #999999;">#### model save ####</span>
models[<span style="color: #993366;">1</span>][<span style="color: #993366;">1</span>].fit(X, Y)
filename = <span style="color: #3b9931;">'titanic_survival_prediction_%s.sav'</span> % models[<span style="color: #993366;">1</span>][<span style="color: #993366;"></span>]
dump(models[<span style="color: #993366;">1</span>][<span style="color: #993366;">1</span>], filename)</pre>

<p>Â </p>

<p><img class="aligncenter size-full wp-image-1202" src="/images/2018/10/no-name-1.jpg" alt="" width="618" height="181" srcset="/images/2018/10/no-name-1.jpg 618w, /images/2018/10/no-name-1-300x88.jpg 300w" sizes="(max-width: 618px) 100vw, 618px" /></p>

<p>ìœ„ëŠ” featureì˜ correlation ì •ë³´ì´ë‹¤. ì ˆëŒ€ê°’ì´ 1ì´ë©´ ì™„ë²½í•œ correlation, 0ìœ¼ë¡œ ê°ˆìˆ˜ë¡ ì„œë¡œ ì—°ê´€ì´ ì—†ë‹¨ ì†Œë¦¬ë‹¤.</p>

<p>Survivedì™€ ê¹Šê²Œ ê´€ë ¨ëœ featureëŠ” Pclass, Sex, Fare ì •ë„ë¡œ ë³´ì¸ë‹¤. ë‚¨ì„±ë³´ë‹¨ ì—¬ì„±ì´, ê·¸ë¦¬ê³  ë” ì¢‹ì€ ë“±ê¸‰ì˜ Fareë¥¼ ê°€ì§„ ê³„ê¸‰ì˜ ì‚¬ëŒì´ í†µê³„ì ìœ¼ë¡œ ë” ë§ì´ êµ¬ì¶œë˜ì—ˆë‹¨ ì†Œë¦¬ë‹¤.</p>

<p>ê·¸ëŸ°ë° Raw Dataë¥¼ ë¶ˆëŸ¬ì˜¤ë©´ Characterë¡œ ëœ featureê°€ ìˆë‹¤. ì˜ˆë¥¼ë“¤ì–´ Sex ê°™ì€ ê²½ìš°ëŠ” ë‚¨ìëŠ” â€˜maleâ€™, ì—¬ìëŠ” â€˜femaleâ€™ì´ë¼ëŠ” ë¬¸ìì—´ë¡œ ë˜ì–´ìˆëŠ”ë° map í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ìˆ«ì ë°ì´í„°ë¡œ ë³€í™˜í•´ì•¼í•œë‹¤. ë˜í•œ NaN ìœ¼ë¡œ í‘œì‹œë˜ëŠ” ì •ë³´ê°€ ë¹„ì–´ìˆëŠ” ë°ì´í„°ê°€ ìˆëŠ”ë° ì´ê²ƒì€ ê° featureì˜ median ê°’ìœ¼ë¡œ ì±„ì›Œë„£ì—ˆë‹¤. pandas ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ fillna í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´ ê°€ëŠ¥í•˜ë‹¤.</p>

<p><img class="aligncenter size-full wp-image-1205" src="/images/2018/10/no-name-3.jpg" alt="" width="431" height="161" srcset="/images/2018/10/no-name-3.jpg 431w, /images/2018/10/no-name-3-300x112.jpg 300w" sizes="(max-width: 431px) 100vw, 431px" /></p>

<p>ì•„ë¬´íŠ¼ Logistic Regression, K-Nearest Neighbors, Random Forest 3ê°œì˜ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ê²°ê³¼ë¥¼ êµ¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. cross validation ì ìˆ˜ì™€ test dataì˜ ì˜ˆì¸¡ ê²°ê³¼ê°€ ì¡°ê¸ˆ ë‹¤ë¥´ë‹¤. ë‹¨ìˆœíˆ dataì˜ ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥í•¨ì˜ ìš°ì—°ì—ì„œ ì˜¨ ê²ƒì¸ì§€ bias-variance trade-offì— ì˜í•œ ê²°ê³¼ì¸ì§€ëŠ” ì¡°ê¸ˆ ë” ë´ì•¼ ì•Œê² ë‹¤.</p>

<p>ì´ ë¬¸ì œë¥¼ ì´í›„ë¡œë„ ì—¬ëŸ¬ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ parameter tuningê³¼ ë°ì´í„° ì „ì²˜ë¦¬ ìŠ¤í‚¬ì„ ëŠ˜ë¦° í›„ accuracy ë†’ì´ê¸°ì— ë„ì „í•´ ë³¼ ìƒê°ì´ë‹¤.</p>

<p>Â </p>

<p>Â </p>
:ET